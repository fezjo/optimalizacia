\section{The primal-dual method}

\noindent
We hope to have persuaded the reader that duality is an interesting property of linear programs. 
Now it is time to show that it is also a useful one. Let us consider the primal-dual pair of
linear programs

\begin{eqnarray*}
  (P):&\min\limits_{\bm{x}\in\R^n}\{\bm{c}\tr\bm{x}\mid A\bm{x}\ge\bm{b}, \bm{x}\ge 0\}\\
  (D):&\max\limits_{\bm{y}\in\R^m}\{\bm{b}\tr\bm{y}\mid A\tr\bm{y}\le\bm{c}, \bm{y}\ge 0\}
\end{eqnarray*}

\noindent
From the strong duality theorem we know that they have the same value of the optimum, i.e. there
exist vectors  $\bm{x^\star}\ge0$
and $\bm{y^\star}\ge0$, such that $A\bm{x^\star}\ge\bm{b}$, $A\tr\bm{y^\star}\le\bm{c}$
and $\bm{c}\tr\bm{x^\star}=\bm{b}\tr\bm{y^\star}$.
Recall the inequalities from the proof of Theorem\ref{thm:weakduality} that hold for
all pairs of feasible solutions of a primal-dual pair, and so in particular for $\bm{x^\star}$, $\bm{y^\star}$:

\begin{equation}
  \label{eq:proofSlack}
  \bm{c}\tr\bm{x^\star}\stackrel{(\clubsuit)}{\ge}\left(A\tr\bm{y^\star}\right)\tr\bm{x^\star}
=\bm{y^\star}\tr A\bm{x^\star}\stackrel{(\diamondsuit)}{\ge}\bm{y^\star}\tr\bm{b}
\end{equation}

\noindent
Since $\bm{c}\tr\bm{x^\star}=\bm{b}\tr\bm{y^\star}$, both $(\clubsuit)$ and $(\diamondsuit)$ 
must be equal. Let us have a look at the  $(\clubsuit)$, and expand the scalar product of vectors into a sum:

\begin{equation}
  \label{eq:kompl:1}
  \sum_{j=1}^n c_jx^\star_j=\sum_{j=1}^n\left[A\tr\bm{y^\star}\right]_jx^\star_j,
\end{equation}

\noindent
where the symbol $[\cdot]_j$ denotes the  $j$-th coordinate of a vector. 
From the feasibility of the dual solution we know that
$\bm{c}\ge A\tr\bm{y^\star}$; 
the inequality of the vectors holds in every coordinate, so for each $j$ it is
$c_j\ge\left[A\tr\bm{y^\star}\right]_j$.
For each $j$ there is $x^\star_j\ge0$, and so also  $c_jx^\star_j\ge\left[A\tr\bm{y^\star}\right]_jx^\star_j$,
which implies that in order the equality (\ref{eq:kompl:1}) to hold, the equality must hold separately in every 
coordinate.


\noindent
The same reasoning goes for the inequality  $(\diamondsuit)$, yielding the following characterization
of the optimum solutions:

\begin{framed}
\begin{veta}[slackness conditions]
  \label{thm:slackness}
Let \bm{x}, \bm{y} be feasible solutions of the primal and dual programs, respectively.
Then \bm{x}, \bm{y} correspond to the common optimal solution if and only if the following 
conditions hold:

\begin{itemize}
\item primal slackness conditions:
$$\forall\;1\le j\le n:\;{\rm\ either\ }\; x_j=0\; {\rm\ or\ }\sum_{i=1}^ma_{ij}y_i=c_j$$
\item dual slackness conditions:
$$\forall\;1\le i\le m:\;{\rm\ either\ }\; y_i=0\; {\rm\ or\ }\sum_{j=1}^na_{ij}x_j=b_i$$
\end{itemize}
\end{veta}
\end{framed}

\subsection*{Edmonds' algorithm for \minfactor}

\noindent
When we spoke about the simplex method of solving linear programs, we argued that it is an efficient, albeit 
not polynomial-time, algorithm. Also, we mentioned that there exist polynomial methods for solving LP. The
truth is that, polynomial or not, solving LP can be quite expensive, especially if the program is large. 
The slackness conditions present a tool to significantly reduce the complexity of LP-based algorithms:
since we have an equivalent characterization of the optimal solution, we don't have to solve the LP. 
Instead, if we construct, using an arbitrary algorithm, solutions of the primal and dual programs that
satisfy the slackness conditions, we know that we have optimum solution. The usual approach in the
primal-dual method is to maintain a pair of solutions of the primal-dual pair of programs, such that
we start with a good (but not feasible) primal, and a feasible (but far from optimum) dual solutions.
In a sequence of iterations, we improve the value of the dual solution while maintaining feasibility, 
and at the same time adjust the primal
solution so that it is ''closer'' to being feasible.  In the end, we have a feasible primal-dual pair, 
with hopefully the same (optimal) value. We shall illustrate this approach on an example.
Recall the problem \maxWBmatching:

{
  \renewcommand{\thedummy}{\ref{dfn:maxWBmatching}}
  \begin{dfn}
    Given a bipartite graph with edges labeled by non-negative weights, the\\ \maxWBmatching
    problem asks to select a set of edges with maximal overall weight in such a way that no
    two selected edges share a common vertex.
  \end{dfn}
}


In section \ref{sec-ilp} we formulated the problem as an ILP:

\begin{equation*}
\begin{array}{rrcll}
  {\rm maximize}     & \multicolumn{1}{l}{\sum\limits_{e\in E}\omega_ex_e}\\[3ex]
  {\rm subject\ to} & \sum\limits_{e\in E\atop e=(v,w)}x_e&\le&1& \;\;\;\forall v\in V\\
                          & x_e&\ge&0& \;\;\;\forall e\in E\\
                          & x_e&\in&\Z
\end{array}
\end{equation*}

\noindent
where $\bm{\omega}\in\R^n$ is the vector of edge-weights. We showed that the matrix of constraints is TUM,
and thus it is sufficient to solve the relaxed program, and we have the integrality of the optimum
solutions for free. Quite naturally, we can ask: What happens if the graph is not bipartite? The formulation
of the ILP is still valid (we haven't used the fact that the graph was bipartite), but it is not true
anymore that the optimum solution is always integral.

\begin{myfig}{0.8\textwidth}{svg/matchingLP}
  On the left there is a graph with its edge-weights where the maximum matching has value $21$:
  edges of weight 10 are only among vertices $\{p,q,r,s,t\}$, so in any solution there can be at most two of them.
  From the remaining two edges, any solution can contain only one. On the right there is a solution of
  the relaxed program with value $25$.
\end{myfig}

\noindent
Before we continue we reformulate our problem slightly. A matching that covers all vertices, i.e. the 
set of edges $E'\subseteq E$ such that every vertex is incident with exactly one edge from $E'$ will
be called a {\em perfect matching} or a {\em 1-factor}. Obviously, in order a graph could have a 1-factor, it
has to have even number of vertices. Instead of finding the heaviest matching, it is sufficient to be able
to find the heaviest 1-factor: we first adjust the input graph $G$ such that if it has odd number of vertices
we add a new isolated vertex, and then we connect all pairs of vertices that are not connected by an existing
edge, by new edges with weight 0. It is easy to check that matchings in $G$ correspond to 1-factors in $G'$,
and vice-versa. Next, if $\omega_{\max}$ is maximum weight of an edge in $G'$. If we replace the weights
of all edges from $\omega_e$ to $\omega_{\max}-\omega_e$, we get the following definition:

\begin{framed}
  \begin{dfn}
    \label{dfn:minFactor}
    Given is a complete graph $G=(V,E)$ with even number of vertices and non-negative edge weights
    $\omega_e\in\R^+$. The problem \minfactor is to find a 1-factor of $G$ that minimizes the
    sum of weights, i.e. with weight $\min_{E'}\sum_{e\in E'}\omega(e),$ where the minimum is taken
    over all 1-factors of $E'$ of $G$.
  \end{dfn}\end{framed}


\noindent
The reader who was patient enough to bear with us up to now can easily express \minfactor as an ILP:
for each edge $e\in E$ we introduce a variable $x_e\in\{0,1\}$ denoting whether $e$ is selected in the matching
or not. The selected set of edges is a 1-factor exactly if there is exactly one selected edge incident to
any vertex; this can be expressed in a straightforward way

\begin{equation}
  \label{eq:1f:ILP}
\begin{array}{rrcll}
  {\rm minimize}     & \multicolumn{1}{l}{\sum\limits_{e\in E}\omega_ex_e}\\[3ex]
  {\rm subject\ to} &  \sum\limits_{e\in E\atop e=(u,v)} x_e &=&1& \;\;\;\forall v\in V\\
                          & x_e&\in&\{0,1\}& \;\;\;\forall e\in E\\
\end{array}
\end{equation}

\noindent
If we relax this program by considering $x_e\ge0$ instead of  $x_e\in\{0,1\}$ (note that  $x_e\le1$
is implied by the minimization), the optimum is not necessarily integral. We shall now present the
primal-dual approach according to Edmonds \cite{Edmonds65}. Before that, however, we introduce one more
notation that simplifies the subsequent text:


\begin{dfn}[edge boundary of a set of vertices]
  \label{dfn:edgeboundary}
Given a  graph  $G=(V,E)$, and a set of vertices $S\subseteq V$, the edge boundary of the set $S$,
denoted $\partial(S)$, is the set of edges with one endpoint in $S$, and the other outside of $S$, 
i.e.
$$\partial(S):=\{e\in E\mid e=(u,v),\;u\in S,\;v\in V\setminus S\}$$
\end{dfn}

\noindent
How should we cope with the fact that the program  (\ref{eq:1f:ILP}) has no integral optimum? The main
idea is to augment the program with many additional constraints that have no impact on the ILP, but they
force the relaxed program to have integral optimum. Let \S be the family of all sets of vertices 
with odd size, containing at least three vertices, i.e.

$$\S:=\left\{ S\subseteq V\mid\; |S|>1,\;|S|\;{\rm\ odd\ }\right\}$$

\noindent
Every edge has two endpoints, so the vertices from any  $S\in\S$ cannot be paired among themselves, meaning
that in any 1-factor there must be at least one edge outgoing from $S$. We add these constraints explicitly 
to the relaxed program
(\ref{eq:1f:ILP}),
obtaining

\begin{equation}
  \label{eq:1f:P}
\begin{array}{rrcll}
  {\rm minimize}     & \multicolumn{1}{l}{\sum\limits_{e\in E}\omega_ex_e}\\[4ex]
  {\rm subject\ to} &  \sum\limits_{e\in\partial(\{v\})} x_e &=&1& \;\;\;\forall v\in V\\[4ex]
                          & \sum\limits_{e\in\partial(S)}x_e&\ge&1&\;\;\;\forall S\in\S\\[4ex]
                          & x_e&\ge&0& \;\;\;\forall e\in E\\
\end{array}
\end{equation}

\noindent
\ldots and {\em voil√†!} We have a linear program with integral optimum solution. However, now we not only have to 
show that indeed the optimum solution of the program (\ref{eq:1f:P}) is integral, but we have to cope with a 
program that has exponentially many constraints. Neither of these problems is unsurmountable, as there are
methods that allow us to solve, under some assumptions that are satisfied in this case, in polynomial time
even linear programs with exponentially many (in fact, even with infinitely many) constraints. However, we 
have a more clever solution in mind: We avoid solving (\ref{eq:1f:P}) by using duality, and the integrality
proof comes for free. 

\noindent
Let us now use our dualization recipe and write down the dual program to  (\ref{eq:1f:P}).
It is a maximization program with a variable for each constraint of the original program. There are two
types of constraints in the original program: the ones concerning vertices and the ones concerning sets,
so let us introduce two sets of variables: $r_v\in R$ for $v\in V$
and $w_S\in\R^+$ for $S\in\S$.
Every primal variable $x_e$ contributes $x_e\omega_e$ to the utility function, and appears in two 
vertex-constraints (the ones for the endpoints of $e$) and in the set-constraints for the sets 
$S\in\S$ where $e\in\partial(S)$. We obtain the following program (note that $r_v$ may be also negative):

\begin{equation}
  \label{eq:1f:D}
\begin{array}{rrcll}
  {\rm maximize}     & \multicolumn{1}{l}{\sum\limits_{v\in V}r_v + \sum\limits_{S\in\S}w_S}\\[4ex]
  {\rm subject\ to} & 
        r_u + r_v + \sum\limits_{S\in\S\atop e\in\partial(S)} w_S &\le&\omega_e& \;\;\;\forall e=(u,v)\in E\\[4ex]
                          & w_S&\ge&0& \;\;\;\forall S\in\S\\
\end{array}
\end{equation}

\noindent
For the explanatory reasons, and also because in our pair of programs not all variables are non-negative,
let us recall the inequality  (\ref{eq:proofSlack}):

$$
\sum_{e\in E}x_e\omega_e
\stackrel{\clubsuit}{\ge}
\sum_{e\in E\atop e=(u,v)}x_e(r_u+r_v+\sum_{S\in\S\atop e\in\partial(S)}w_S)
\stackrel{\heartsuit}{=}
\sum_{v\in V}(r_v  \textcolor{blue}{\sum_{e\in\partial(\{v\})}x_e} )   + \sum_{S\in\S}w_S
\textcolor{red}{\left(\sum_{e\in\partial(S)}x_e\right)}
\stackrel{\diamondsuit}{\ge}
\sum_{v\in V}r_v+\sum_{S\in\S}w_S
$$


\noindent
The equality in $(\heartsuit)$ comes due to the fact that every vertex $v\in V$
contributes by $r_vx_e$ to all edges $e$ incident with $v$, and every set $S\in\S$ 
contributes by $w_sx_e$ to all edges from the edge boundary of $S$. The constraints of the program 
(\ref{eq:1f:P}) imply that the blue sum $\sum_{e\in\partial(\{v\})}x_e=1$ and 
the red sum $\sum_{e\in\partial(S)}x_e\ge1$; hence, the slackness conditions are of the form


$$\begin{array}{lrl}
  {\bf S1} (\clubsuit) \;\;&\forall e=(u,v)\in E:\;\;&x_e>0\Rightarrow r_u + r_v +
  \displaystyle\sum\limits_{S\in\S\atop e\in\partial(S)}w_S=\omega(e)\\[6ex]
  {\bf S2} (\diamondsuit)\;\;&\forall S\in\S:\;\; & w_S>0\Rightarrow \displaystyle\sum\limits_{e\in\partial(S)}x_e=1
\end{array}$$


\noindent
Now have a look at the program (\ref{eq:1f:D}) and try to find some intuitive interpretation. Imagine that there is a 
bubble around every vertex $v\in V$, and every set $S\in\S$, carrying a charge of $r_v$, and $w_S$, respectively.
Clearly, the sets $S\in\S$ may overlap, making the image of bubbles sort of weird, however, in the final algorithm
we end up using only selections of non-overlapping bubbles. The program (\ref{eq:1f:D}) aims at maximizing
the overall charge. The edge weights can be considered a capacity, and the constraints require that no edge $e$
is overcharged: the overall charge on all bubbles that $e$ crosses cannot be more than its capacity.
An edge $e$ for which  $ r_u + r_v + \sum_{S\in\S\atop e\in\partial(S)}w_S=\omega(e)$ will be called {\em full}.
The conditions  {\bf S1} and {\bf S2} can, together with the strong duality theorem, provide the following 
observation

\begin{lema}
  \label{lm:1f:opt}
  If we find a 1-factor $M$, and the assignment of the charge to the bubbles  \bm{r} and \bm{w} 
  in such a way that
  \begin{itemize}
    \item[{\bf (I1)}] no edge is overcharged,
    \item[{\bf (I2)}] all $w_S\ge 0$,
    \item[{\bf (I3)}] all edges from $M$ are full, and
    \item[{\bf (I4)}] from each non-zero bubble  $S\in\S$ there is exactly one outgoing edge from $M$,
\end{itemize}
  then we have an optimal solution of the primal-dual pair of programs (\ref{eq:1f:P}) and (\ref{eq:1f:D},
  with an integral value of \bm{x}, and thus a minimum 1-factor.
\end{lema}

\begin{dokaz}
  The conditions {\bf (I1)} and {\bf (I2)} guarantee the feasibility of the dual program  (\ref{eq:1f:D}).
  The fact that $M$ is a 1-factor guarantees the feasibility of the primal program  (\ref{eq:1f:P}).
  The conditions  {\bf (I3)} and {\bf (I4)} are, respective, the slackness conditions {\bf S1} and {\bf S2},
  so the statement is a corollary of Theorem~\ref{thm:slackness}.
\end{dokaz}

\noindent
From this moment on we can forget about the whole linear programming business, and concentrate on 
finding an algorithm that constructs the desired objects $M$, \bm{r}, and \bm{w}. As we already hinted,
since the whole vector \bm{w} is too big, we shall explicitly store only the non-zero entries; at the same
time, we shall be looking only for solutions where the sets with non-zero bubbles do not intersect each other. 
We are seeking the following structure:

\vbox{
\begin{myfig}{0.8\textwidth}{svg/edmondsOverview}
  The blue bubbles have all value $1/2$, the edges that are not drawn have weight $\infty$. The red edges form a 
  1-factor. No edge is overcharged, all red edges are full, and there is exactly one outgoing red edge from 
  each green bubble. Hence, we know that the red 1-factor is minimum.
\end{myfig}
}


\noindent
Of course, so far we have no guarantee that such a configuration always exists. However, if we prove it, and 
find an algorithm that constructs it, we can check the task as finished and celebrate.


\noindent
Let us begin with an informal description of the algorithm. During the whole computation, the 
conditions  {\bf (I1)}, {\bf (I2)} and {\bf (I3)} will be maintained as invariants. The algorithm starts with
an empty matching $M$ and all bubbles zero. Gradually, the charge of some bubbles will be increased, and 
some edges will be added to the matching so that, in the end, $M$ is 1-factor, and the condition  {\bf (I4)}
holds; then $M$ is the minimum 1-factor. In the first step the charge is put to all bubbles $r_v$ simultaneously
(in the rest of the description, we shall call these bubbles {\em blue}, whereas the bubbles
$w_S$ will be called {\em green}). The first step ends when no more charge can be put to blue bubbles, 
because some edge $e=(u,v)$ became full. The edge $e$ is then added to the matching $M$, and the bubbles
$r_u$ and $r_v$ won't be incremented in the next step: they will form a {\em dumbbell}. Ultimately, 
an edge $e$, whose one endpoint is a vertex that is already incident with an edge from $M$, becomes full.
This edge is full, meaning that the bubbles at its ends cannot be increased, however, it cannot be added to $M$.
We shall call it a  {\em blocking} edge, and the algorithm shall maintain a set of blocking edges $L$.



\begin{myfig}{0.6\textwidth}{svg/edmonds1-en}
\end{myfig}

\noindent
Apart from free bubbles and dumbbells, a new structure has just formed: a path of length 3 composed of 
a blocking edge from $L$, and a matching edge from $M$; paths containing alternating edges from $L$ and $M$
will be called {\em alternating paths}. If, in the next iteration, the charge of the {\em free } bubbles 
will be increased by $+\varepsilon$, the odd and even bubbles from the alternating paths will receive 
the change of $+\varepsilon$ and $-\varepsilon$, respectively, as to not overcharge any edge; recall that,
unlike the green bubbles, the blue bubbles can have negative charge. In the next iteration, edges connecting
to bubbles where the charge is being increase may become full, which gives rise to trees of the alternating paths
(called {\em Hungarian trees}). A welcomed case is if the newly filled edge creates an alternating path with odd 
number of vertices, in this case the matching can be increased by swapping the membership of $L$ and $M$ edges 
along the path, and instead of an alternating path we have a set of dumbbells.


\begin{myfig}{0.9\textwidth}{svg/edmonds2-en}
\end{myfig}

\vspace*{-5ex}
\noindent
What should we do, however, if an edge becomes full, that connects two bubbles in a single tree? This is
the moment when the time has come for the green bubbles to play their part, and to the hierarchic structures
of bubbles.

\vskip 2ex
\noindent
The basic data structure of the algorithm is a {\em flower}. Each flower has an outer bubble, and inside it
a distinguished vertex called {\em stem}. The most simple flower is a single vertex surrounded by a blue bubble.
More complex flowers are formed by induction: take an odd number of flowers $K_1,K_2,\ldots,K_{2r+1}$, $r\ge 1$ 
(i.e. at least three flowers) such that the stems of the flowers $K_{2i}$ and $K_{2i+1}$ for $i=1,\ldots,r$
are connected by an edge from $M$, and at the same time for each pair of flowers 
$A:=K_{2i-1}$ and $B:=K_{2i}$ for $i=1,\ldots,r$, and $A:=K_{2r+1}$ and $B:=K_1$, there exist vertices 
$u\in A$ and $v\in B$, such that the edge $(u,v)\in L$. Then the green bubble surrounding  $K_1,\ldots,K_{2r+1}$ 
created a new flower, whose stem is the stem of the flower $K_1$. Flowers that are not parts of another flower
are called {\em outer flowers}.

\begin{myfig}{0.7\textwidth}{svg/edmonds3}
  \centerline{Examples of flowers with stems denoted by a square. The red edges are from $M$, 
  the black ones are from $L$.}
\end{myfig}

\noindent
It is worth noting that a flower encapsulates a part of the graph that is ''almost done'': with the exception of the 
stem, all vertices of the flower are pairwise matched by edges from $M$. At the same time 
exactly one edge from $M$ leaves any bubble in flower that 
does not contain the stem; from the bubbles
that contain the stem, on the other hand, leaves no edge from $M$. The previous observation is crucial for
the understanding of the algorithm, and we recommend the reader to devise a detailed proof of it using induction.
If there is a full edge connecting stems of two flowers, we can add it to $M$, and obtain a dumbbell. If all
vertices of the graph are included in dumbbells, the algorithm is finished.


\noindent
\begin{minipage}[t]{0.45\textwidth}
  \vskip 0pt
\begin{myfig}{\textwidth}{svg/edmonds4}
  \centerline{ The {\em shift} operation on a Hungarian tree.}
\end{myfig}
\end{minipage}
\hfill
\begin{minipage}[t]{0.5\textwidth}
  \vskip 0pt

\noindent
Free flowers that do not form dumbbells are organized in Hungarian trees. The flower on level 0 is {\em root} 
of the tree, the stem of a flower on level $2h-1$ is connected by an edge from $M$ with the stem of a flower
of level $2h$. If $K$ is a flower on level $2h$, and $H$ is a child of $K$ in the tree, then there is
an edge from $L$ connecting some vertex from $K$ to some vertex from $H$. The intuition is as follows:
the root of the tree is a flower that we would like to include in a dumbbell, but in order to do so, we
need a full edge connecting its stem to the stem of some other flower. However, there is no such edge, and
we cannot add more charge to the outer bubble of the flower to make some edge full, because there are other
edges crossing the outer bubble that are already full: the blocking edges from $L$; these edges lead to 
the flowers that are the children of the root, and so on. 

\noindent
During its computation, the algorithm maintains a set
of Hungarian trees, and the rest of the graph is covered by dumbbells. The algorithm 
works in iterations: in one iteration it performs a {\em shift} operation on all trees. The operation
adds some $\varepsilon$ charge to the outer bubbles of all flowers on 
\end{minipage}

\noindent
even levels, and subtracts the
same value from the outer bubbles of flowers on odd levels. The value of $\varepsilon$ is chosen as the
biggest value that does not violate the conditions {\bf (I1)}, {\bf (I2)}. The conditions may 
be violated by several ways:

\noindent
{\bf (P1) The charge of a green bubble on odd level dropped to 0. }
Let $K$ be the flower whose outer bubble reached zero charge. By the definition of a flower, $K$
contains an odd number of flowers $K_1,\ldots,K_{2r+1}$, such that the stem of $K$ is in  $K_1$.
Since $K$ is on odd level in the tree, $K$ has one parent, and (exactly) one child connected
stem-to-stem by an edge from $M$. Let the edge from $L$ connecting $K$ to its parent is from some
vertex in the flower $K_t$, and without loss of generality, let $t$ be odd. Then the
path $K_1,K_2,\ldots,K_t$ contains an odd number of flowers and can replace $K$ in the tree.
The pairs of flowers $K_{t+1},K_{t+2}$ up to $K_{2r},K_{2r+1}$ form dumbbells. The full edges
between $K_t$ and  $K_{t+1}$, and $K_1$ and $K_{2r+1}$ can be removed from $L$, since in the 
new tree, each of them has one endpoint on an odd level, and the other one in a dumbbell, so in the
next iteration they will not be full anymore. 

\begin{myfig}{0.8\textwidth}{svg/edmonds5}
\end{myfig}

\vspace*{-4ex}
\noindent
{\bf (P2) An edge $e$ connecting a flower $K$ on an even level, and a dumbbell $H$, has become full.} 
Let the dumbbell $H$ consist of flowers $H_1$ and $H_2$, so that $e$ leads to some vertex in $H_1$. The
edge $e$ will be included in $L$, and $H$ will join the corresponding tree so that $K$ (on even level)
will have a child $H_1$ (on odd level), and $H_1$ will have a child $H_2$ (on even level).

\begin{myfig}{0.7\textwidth}{svg/edmonds6}
\end{myfig}

\vspace*{-4ex}
\noindent
{\bf (P3)  An edge $e$ connecting flowers $K$ and $H$ in one tree has become full.}
Clearly, both $K$ and $H$ are on even level. Let $W$ be the closest common ancestor of $K$ and $H$. Since $W$
has at least two children, it must be also on an even level. Let  $K,K_1,\ldots,K_{2k+1},W$ and
$H,H_1,\ldots,H_{2r+1},W$ be two paths in the tree. From the parity of their  lengths it follows that they
can be wrapped in a new outer bubble to obtain a flower $Z$ on even level, whose stem is the stem of $W$.
The children of $Z$ are all the children of included flowers, they remain on odd levels. 

\begin{myfig}{0.8\textwidth}{svg/edmonds7}
\end{myfig}

\vspace*{-4ex}
\noindent
{\bf (P4) An edge $e$ connecting flowers $K$ and $H$ in two different trees ${\cal T}_1$ and ${\cal T}_2$ 
has become full.}
This is actually the core of the algorithm where the matching $M$ is increased. It is done by finding 
an alternating path connecting the stem of the root of ${\cal T}_1$ with the stem of the root of 
${\cal T}_2$. Since an edge became full, both flowers $K$ and $H$ are on even levels in their respective trees,
meaning that there is a path $K,K_1,\ldots,K_{2r}$ in ${\cal T}_1$, and a path $H,H_1,\ldots,H_{2q}$ 
in the tree ${\cal T}_2$, where $K_{2r}$ and $H_{2q}$ are roots of the respective trees. 
Both paths are formed by the edges of the trees, which are also edges of the original graph $G$, and
edges belonging to $L$ and $M$ are alternating on them, such that an edge from $M$ connects 
stems of neighboring flowers. In order to augment this alternating path in the trees
to an alternating path in the graph $G$ we observe the following:

\begin{lema}
  \label{lm:1f:tmp1}
  Let $K$ be a flower with a stem  $u$, and let $v$ be an arbitrary vertex from $K$. Then there
  exists an alternating path in $G$ from $u$ to $v$ fully contained in $K$, and if this path is
  non-empty it starts with an edge from $L$ and ends with an edge from $M$.
\end{lema}


\begin{dokaz}
  We prove the lemma by induction on the depth of recursion of the flower. For a flower with a single
  vertex the statement holds trivially. Further, let $K$ be a flowers composed of sub-flowers
  $K_1,\ldots,K_{2r+1}$ such that the stem of $K$ is the stem of $K_1$. Without loss of generality
  let $v\in K_{2t-1}$ for some $t$ (if $v\in K_{2t}$ we just change the direction of the
  numbering of the sub-flowers). If $t=1$ we can use the induction hypothesis on the flower $K_1$.
  So let us assume $t>1$. From the definition of the flower we can infer the existence of an edge
  $(q,w)\in L$ such that $q\in K_1$ and $w\in K_2$. By induction, there is an alternating $u-q$ path in $K_1$
  that ends by an edge from $M$. At the same time there is an alternating path
  in $K_2$ from the stem to $w$ that ends by an edge from $M$. Joining these two paths together, we get
  an alternating path from $u$ to the stem of $K_2$ that ends by and edge from $L$, and so can be extended
  to the stem of $K_3$. Iterating this construction we obtain an alternating path to the stem of $K_{2t-1}$
  that can, again by the induction hypotheses, be extended to $v$. 
\end{dokaz}


\noindent
The construction from Lemma~\ref{lm:1f:tmp1} 
enables us to find an alternating path from the stem of $K_{2r}$ and the stem of $K_{2q}$. Along this path, 
we swap the membership of edges to $L$ and $M$, thus increasing the number of edges in $M$ by one. Subsequently,
the trees ${\cal T}_1$ and ${\cal T}_2$ can be dismantled: the flowers $K_{2i-1}$ and $K_{2i}$ (and, similarly,
$H_{2j-1}$ and $H_{2j}$) form after the swap a set of dumbbells. To see this it is sufficient to note that
the path from the proof of Lemma~\ref{lm:1f:tmp1} either does not cross a given flower, or crosses it 
exactly twice, once by an edge from $M$, and once by an edge from $L$. Hence, after the swap we keep the invariant
that every bubble is crossed by exactly one edge from $M$, and the stem of the tree is moved to the vertex $v$ 
specified by Lemma~\ref{lm:1f:tmp1}. Similarly, the flowers $K$ and $H$ form a dumbbell, and the 
remaining subtrees of both  ${\cal T}_1$ and ${\cal T}_2$ form dumbbells in a natural way.

\begin{myfig}{\textwidth}{svg/edmonds8}
\end{myfig}


\noindent
The final algorithm works in a cycle: while $M$ is not a 1-factor, one iteration finds the smallest
value $\varepsilon$ such that the shift by $\varepsilon$ violates some of the conditions  {\bf(I1)},  {\bf(I2)} 
from Lemma~\ref{lm:1f:opt}. According to what situation occured, one of the actions  {\bf(P1)},  {\bf(P2)}, {\bf(P3)} 
or {\bf(P4)} is executed, and a next iteration begins. The condition {\bf (I3)} remains true as an invariant.
When the algorithm terminates, $M$ is a 1-factor, and no flower can be a root of a tree (it would mean
its stem is not matched in $M$, so $M$ would not be a 1-factor); hence, all vertices are included in dumbbells,
and the condition {\bf (I4)} holds, too. Lemma~\ref{lm:1f:opt} then implies that when the algorithm terminates,
$M$ is a minimum 1-factor.

\noindent
The only thing to show now is that the algorithm indeed terminates, and if possible also that it terminates soon
enough. Our argument is based on the following observation:

\begin{lema}
  The described algorithm for the \minfactor problem performs at most $O(n^2)$ iterations on an $n$-vertex graph $G$.
\end{lema}
\begin{dokaz}
  The size of $M$ never decreases, and is increased by 1 in every execution of the action {\bf (P4)}.
  Hence, the algorithm may perform at most $O(n)$ times the action  {\bf (P4)}. In order to prove the statement
  it is sufficient to show that between two consecutive executions of the action {\bf (P4)} there are at most
  $O(n)$ iterations of the algorithm, performing actions  {\bf (P1)}, {\bf (P2)} and {\bf (P3)}.


\noindent
  The first thing to note is that at any point during the execution there are at most $O(n)$ bubbles
  overall (including the bubbles inside flowers). This is due to the fact that every flower contains at least three
  sub-flowers, and so if we call the {\em depth} of the flower the length of a maximum chain of included bubbles,
  then at any time the algorithm can maintain at most $n$ flowers of depth 0, $n/3$ flowers of depth 1, $n/3^2$
  flowers of depth 2, and so on, forming a geometric series.


\noindent
 Now let us consider the computation of the algorithm between two executions of the action  {\bf (P4)}.
 Another important observation is that the charge of an outer bubble of a flower on even level will never decrease:
 either it remains as an outer bubble of a flower of even level, or it becomes part of another flower
 of even level in action  {\bf(P3)}).  We shall call as {\em safe bubble} a bubble that was, at some point
 during the considered period of computation between two executions of  {\bf (P4)}, an outer bubble
 of a flower on even level. To finish the proof it is sufficient to show that actions  {\bf (P1)}, {\bf (P2)} and
 {\bf (P3)} increase the number of safe bubbles. <since the safe bubbles never disappear and there are linearly 
 many bubbles overall, we obtain that between two consecutive executions of  {\bf (P4)}, at most linearly many
 other iterations may occur. 


 \noindent
 The action  {\bf (P1)} destroys a bubble $B$ on odd level, and at least one bubble $B'$ from its inside
 appears on an even level. This bubble, however, was not safe before, since a safe bubble never becomes a part
 of a flower on odd level. The action  {\bf (P2)} adds a safe bubble from the dumbbell, and the action
 {\bf (P3)}  creates a new safe bubble.
\end{dokaz}


\noindent
A straightforward implementation of one iteration runs in time $O(nm)$, where $n$ is the number of vertices, and $m$
is the number of edges: for each edge, check all bubbles and check what largest $\varepsilon$ this edge still allows.
Select the edge with smallest bound, and perform the corresponding action. Hence, we can say:

\begin{veta}
  The \minfactor problem is solvable in time $O(n^3m)$.
\end{veta}

\noindent
For an insightful reader we may add that using a more clever data structures, the running time can be significantly
improved; it is, however, beyond the scope of this text.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Relaxed slackness conditions}


\noindent
In the previous part we introduced the primal-dual method based on the characterization of optimal solutions
by slackness conditions. For a  primal-dual pair of programs

\begin{eqnarray*}
  (P):&\min\limits_{\bm{x}\in\R^n}\{\bm{c}\tr\bm{x}\mid A\bm{x}\ge\bm{b}, \bm{x}\ge 0\}\\
  (D):&\max\limits_{\bm{y}\in\R^m}\{\bm{b}\tr\bm{y}\mid A\tr\bm{y}\le\bm{c}, \bm{y}\ge 0\}
\end{eqnarray*}

\noindent
the vectors $\bm{x}$ and $\bm{y}$ are optimal solutions of $(P)$ and $(D)$, respectively, if and only if

\begin{eqnarray*}
  \forall\;1\le j\le n:\;{\rm\ either\ }\; x_j=0\; {\rm\ or\ }\sum_{i=1}^ma_{ij}y_i=c_j\\
  \forall\;1\le i\le m:\;{\rm\ either\ }\; y_i=0\; {\rm\ or\ }\sum_{j=1}^na_{ij}x_j=b_i
\end{eqnarray*}

\noindent
Now we want to use this method to obtain an approximate solution of some \NP-hard optimization
problems. Can this characterization be of any help? Can we, for example, say that if we have a pair of
vectors \bm{x}, \bm{y} that violate the slackness conditions, but only a ``little bit'', then we can
obtain a solution that is ``almost'' optimal? Indeed, we can modify the Theorem~\ref{thm:slackness}
as follows:


\begin{framed}
\begin{veta}
  \label{thm:slacknessrelax}
   Let \bm{x} and \bm{y} be feasible solutions of program $(P)$ and $(D)$, respectively, and let it, for some 
   $\alpha,\beta\ge1$, hold

\begin{eqnarray*}
  \forall\;1\le j\le n:\;{\rm\ either\ }\; x_j=0\; {\rm\ or\ }c_j/\alpha\le\sum_{i=1}^ma_{ij}y_i\le c_j\\
  \forall\;1\le i\le m:\;{\rm\ either\ }\; y_i=0\; {\rm\ or\ }b_i\le\sum_{j=1}^na_{ij}x_j\le\beta b_i
\end{eqnarray*}

\noindent
Then $\bm{c}\tr\bm{x}\le\alpha\beta\bm{b}\tr\bm{y}$.
\end{veta}
\end{framed}

\begin{dokaz}
  $$\sum_{j=1}^nc_jx_j\le\sum_{j=1}^n\alpha\left(\sum_{i=1}^ma_{ij}y_i\right)x_j
  \le\alpha\sum_{i=1}^my_i\left(\sum_{j=1}^na_{ij}x_j\right)\le\alpha\beta\sum_{i=1}^my_ib_i
  $$
\end{dokaz}

\noindent
A typical application of this theorem looks like this: we are looking for the smallest integral solution of
$(P)$. If we manage, by some means, to find an integral solution \bm{x}, and some corresponding dual
feasible solution \bm{y} that satisfy the conditions of Theorem~\ref{thm:slacknessrelax} , we are in a situation

\begin{myfig}{\textwidth}{svg/dualrelax-en}
  Both $OPT$ and the found solution $\bm{c}\tr\bm{x}$ are between $\bm{b}\tr\bm{y}$
  and $\alpha\beta\bm{b}\tr\bm{y}$, and so the found solution is at most $\alpha\beta$ times
  the optimum.
\end{myfig}

\noindent
As a concrete example we show (again) the 2-approximation algorithm for \minvcover (recall the
Definition~\ref{dfn:minvcover}). It will, however, be much faster than the algorithm we already provided,
since it will never need to solve any linear program explicitly. The start is the same: we want to 
solve the integer program


\edef\tmp{\theequation}

\setcounterref{equation}{eq:ILP:1}
\addtocounter{equation}{-1}


\begin{equation}
\begin{array}{rrcll}
  {\rm minimize}     & \multicolumn{1}{l}{\sum\limits_{v\in V}\omega_vx_v}\\[3ex]
  {\rm subject\ to} & x_u + x_v&\ge&1& \;\;\;\forall e=(u,v)\in E\\
                          & x_v&\ge&0& \;\;\;\forall v\in V\\
                          & x_v&\in&\Z
\end{array}
\end{equation}

\noindent
We relax it to obtain

\begin{equation}
\begin{array}{rrcll}
  {\rm minimize}     & \multicolumn{1}{l}{\sum\limits_{v\in V}\omega_vx_v}\\[3ex]
  {\rm subject\ to} & x_u + x_v&\ge&1& \;\;\;\forall e=(u,v)\in E\\
                          & x_v&\ge&0& \;\;\;\forall v\in V\\
\end{array}
\end{equation}

\setcounter{equation}{\tmp}

\noindent
Now, however, instead of solving  (\ref{eq:ILP:2}) explicitly, we construct the dual program:

\begin{equation}
  \label{eq:vcdual:1}
\begin{array}{rrcll}
  {\rm maximize}     & \multicolumn{1}{l}{\sum\limits_{e\in E}y_e}\\[3ex]
  {\rm subject\ to} & \sum\limits_{e\in E\atop e=(u,v)}y_e&\le&\omega_u& \;\;\;\forall u\in V\\[3ex]
                          & y_e&\ge&0& \;\;\;\forall e\in E\\
\end{array}
\end{equation}

\noindent
While the program(\ref{eq:ILP:1}) asks to select vertices with minimum weight such that at least
one endpoint of any edge is covered, the program (\ref{eq:vcdual:1}) can be viewed as assigning 
a non-negative charge $y_e$ to each edge $e$, and $\omega_v$ is the capacity of a vertex.
The goal is to put as much charge (weighted by the $\omega_w$) as possible to the graph, but 
no vertex can be overcharged: the sum of charges of edges incident to any vertex $v$ cannot exceed its capacity.
Let us write down the slackness conditions:


$$\begin{array}{lrl}
  {\bf S1} \;\;&\forall v\in V:\;\;&x_v>0\Rightarrow
  \displaystyle\sum\limits_{e\in E\atop e=(u,v)}y_e=\omega_u\\[6ex]
  {\bf S2} \;\;&\forall e=(u,v)\in E:\;\; & y_e>0\Rightarrow x_u+x_v=1
\end{array}$$

\noindent
Following Theorem~\ref{thm:slackness} we can say this: if we can select a set of vertices (i.e. 
integral values of \bm{x}), and put some charge to the edges such that no vertex is overcharged 
(feasible dual solution), every selected vertex is full (slackness conditions {\bf S1}),
and exactly one endpoint is selected from every edge with non-zero charge (slackness conditions 
{\bf S2}), we would have an optimum solution.
Clearly the most problems are caused by the conditions {\bf S2}. However, if we settle 
with an approximation with  $\alpha=1$ and $\beta=2$, we don't have to care about  {\bf S2} at all, 
since $x_u+x_v\le2$ always holds. Theorem~\ref{thm:slacknessrelax} ensures that we have a 2-approximation then.

\noindent
We propose a simple greedy algorithm: it maintains a set of selected vertices $C$, and fore each edge
$e$ its currently assigned charge $y_e$. It starts by assigning $C=\emptyset$ and $y_e=0$ for 
each edge $e$, and continues by a sequence of iterations: while $C$ is not a vertex cover, pick an 
arbitrary edge $e=(u,v)$ that is not covered by $C$, and increase $y_e$ to the maximum value that still
overcharges neither $u$ not $v$. After this increase, obviously, at least one of the 
vertices $u$, $v$ is full, and so some full vertex from $\{u,v\}$ can be entered into $C$. 

\noindent
After the algorithm terminates, it holds that $C$ is a cover: the main cycle of the algorithm does not 
finish while there is some uncovered edge. Moreover, no vertex is overcharged and every selected vertex is full,
since these conditions were maintained as invariants. Hence, the previous theorem implies that the cost of the
found solution
is at most twice the optimum cost. The algorithm works in time $O(n+m)$, where $n$ is the number of vertices
and $m$ is the number of edges.

\section{Weaker than relaxed slackness: \minsforest}

\noindent
We have seen how to use the relaxed version stated in Theorem~\ref{thm:slacknessrelax}
of the slackness conditions to design approximation algorithms.
In this part we introduce an example where we will not be able to ensure the relaxed slackness conditions
to hold, and yet we will be able to design a primal-dual approximation algorithm by arguing about the
slackness conditions ``on average''.

\noindent
Consider the following toy example: given is a railroad network modelled by a graph: vertices represent
cities, and edges are the railroad tracks. A carrier who wants to operate certain connections has to 
rent some tracks from the owner, and for each track there is a fixed non-negative rental cost. The carrier
plans to operate a set of connections between pairs of cities, and seeks to rent tracks that enable
these connections to be operated while minimizing the rental costs. A formal definition could look
like this:


\begin{framed}
  \begin{dfn}
    Given is an undirected graph  $G=(V,E)$ with edge costs $\omega:E\mapsto\R^+$, and a (symmetric)
    mapping $r:V\times V\mapsto\{0,1\}$. The goal of the \minsforest problem is to find a set of
    edges $F\subseteq E$ such that for any pair of vertices $u$, $v$ for which $r(u,v)=1$ there 
    is a $u-v$ path containing only edges from $F$, and the overall cost of edges from $F$ is minimized.
  \end{dfn}
\end{framed}


\begin{myfig}{\textwidth}{ovl/steiner-06}
  An example of a graph with edge costs (blue). The required connections are  $u-v$, $s-t$, $p-q$,
  and $x-z$ (i.e.  $r(u,v)=r(v,u)=r(s,t)=r(t,s)=r(p,q)=r(q,p)=r(x,z)=r(z,x)=1$, and all remaining
  values $r(\cdot,\cdot)$ are zero). The highlighted edges form optimum solution with cost 51.  Note 
  that an optimal solution of \minsforest is always a forest.
\end{myfig}

\noindent
A reader familiar with the theory of \NP-completeness, considers it an easy exercise to show that \minsforest
is an \NP-hard problem, and thus would not expect to find a polynomial-time algorithm that solves it 
optimally. We present here a 2-approximation algorithm, i.e. a polynomial-time algorithm that
always finds a solution with the cost not bigger than twice the cost of the optimum solution.
Let us start as usual by formulating the problem as an ILP. Quite naturally, for each edge $e$ we 
introduce a variable $x_e\in\{0,1\}$ that would denote whether $e$ is selected or not. We need now
to express the connectivity requirements in the form of linear constraints. Since we are about to use the
primal-dual method, we don't care about the size of the program, and we happily introduce exponentially many 
constraints. In order to simplify the exposition, let us call a set $S$ {\em hungry} if there 
exist $u\in S$, $v\in V\setminus S$ for which $r(u,v)=1$, i.e. at least one edge leaving $S$ must be 
included in any feasible solution. We observe the following:

\begin{lema}
  A set of edges $F$ is a feasible solution if and only if for each hungry set $S$ there is at least one edge 
  from $F$ that leaves $S$.
\end{lema}

\begin{dokaz}
  Obviously, in any feasible solution there must be at least one edge leaving any hungry set. 
  In order to prove the opposite direction, consider such set $F$, and arbitrary vertices $u,v\in V$ such that
  $r(u,v)=1$; we find a $u-v$ path in $F$. Let us construct, by induction, a sequence of sets  
  $\{u\}=S_0\subseteq S_1\subseteq\cdots$ with the property that for every vertex $w\in S_i$, there is
  some  $u-w$ path in $F$. For  $\{u\}=S_0$ the statement obviously holds. Take now
  an arbitrary set  $S_i$. If  $v\in S_i$, we have a $u-v$ path from $F$, and we are done. If
   $v\not\in S_i$, then $S_i$ is hungry and so there is some edge from $F$ leaving $S_i$; let this
   edge lead to a vertex  $w\in V\setminus S_i$. Setting $S_{i+1}:=S_i\cup\{w\}$ keeps the property
   that there is an $u-w$ path in $F$ for every vertex $w\in S_{i+1}$ is satisfied.
   Since there are $n$ vertices, and we add one vertex to the set in every step, we eventually reach
   a situation where $v\in S_i$.
\end{dokaz}

\noindent
We shall write $f(S)=1$, if $S$ is hungry, and $f(S)=0$ otherwise. 
Following Definition~\ref{dfn:edgeboundary} let us call  $\partial(S)$
the edge boundary of a set $S$. The problem \minsforest can be expressed as an ILP as follows:

\begin{equation}
  \label{eq:minsforest:ILP}
\begin{array}{rrcll}
  {\rm minimize}     & \multicolumn{1}{l}{\sum\limits_{e\in E}\omega_ex_e}\\[3ex]
  {\rm subject\ to} & \sum\limits_{e\in\partial(S)}x_e&\ge&f(S)& \;\;\;\forall S\subseteq V\\
                          & \multicolumn{3}{r}{x_e\in\{0,1\}}& \;\;\;\forall e\in E\\
\end{array}
\end{equation}

\noindent 
This program can be subsequently relaxed by changing the integrality constraints to  $x_e\ge0$;
the condition $x_e\le1$ is implied by minimization. The relaxed program is

\begin{equation}
  \label{eq:minsforest:P}
\begin{array}{rrcll}
  {\rm minimize}     & \multicolumn{1}{l}{\sum\limits_{e\in E}\omega_ex_e}\\[3ex]
  {\rm subject\ to} & \sum\limits_{e\in\partial(S)}x_e&\ge&f(S)& \;\;\;\forall S\subseteq V\\
                          & x_e &\ge&0& \;\;\;\forall e\in E\\
\end{array}
\end{equation}

\noindent
Now let us construct a dual program to  (\ref{eq:minsforest:P}): we introduce variables $y_S$
for each set  $S\subseteq V$, and write

\begin{equation}
  \label{eq:minsforest:D}
\begin{array}{rrcll}
  {\rm maximize}     & \multicolumn{1}{l}{\sum\limits_{S\subseteq V}y_sf(S)}\\[3ex]
  {\rm subject\ to} & \sum\limits_{S:e\in\partial(S)}y_S&\le&\omega_e& \;\;\;\forall e\in E\\
                          & y_S &\ge&0& \;\;\;\forall S\subseteq V\\
\end{array}
\end{equation}

\noindent
Programs with similar structure have already appeared several times, so we have a natural way to interpret
the program: there is a bubble with some charge around each set. The goal is to maximize the overall charge
of hungry sets (sets that are not hungry do not contribute to the final solution, and we now make a firm resolution
to never ever increase any dual variable belonging to a non-hungry set). The requirement is not to overload
any edge: the sum of the charges on bubbles that a particular edge crosses must not exceed the capacity
of the edge. Note that to increase the charge of a set $y_S$ has the same effect as to increase 
the set $y_{V\setminus S}$. Let us now observe the slackness conditions:


$$\begin{array}{lrl}
  {\bf S1} \;\;&\forall e\in E:\;\;&x_e>0\Rightarrow
  \displaystyle\sum\limits_{S:e\in\partial(S)}y_S=\omega_e\\[6ex]
  {\bf S2} \;\;&\forall S\subseteq V:\;\; & y_S>0\Rightarrow \sum\limits_{e\in\partial(S)}x_e=f(S)
\end{array}$$

\noindent
From the slackness conditions we can conclude that in a hypothetical optimum solution with integral \bm{x},
every selected edge is full, and from every non-zero bubble around a hungry set
leaves exactly one selected edge (we already promised not to increase any bubble around a non-hungry set).

\noindent
Our algorithm will iteratively add edges to the set of selected edges, until all connectivity requirements
are satisfied. At the same time it will maintain invariants that all selected edges are full, and that 
no edge is overloaded. Thus when the algorithm terminates it will have a feasible solution of 
the program (\ref{eq:minsforest:P}), and a feasible solution of the dual program (\ref{eq:minsforest:D}).
Also, the conditions {\bf S1} will be satisfied. If we wanted to use Theorem~\ref{thm:slacknessrelax}
to guarantee the approximation ratio of 2, we would need to ensure, additionally, the conditions

$$\begin{array}{lrl}
  {\bf S2'} \;\;&\forall S\subseteq V:\;\; & y_S>0\Rightarrow \sum\limits_{e\in\partial(S)}x_e\le2f(S),
\end{array}$$

\noindent 
i.e. that from every non-zero bubble (around a hungry set) there are at most two outgoing selected edges.
We will not be able to guarantee this, but as we shall see, a weaker statement that {\em on average}
there are at most two outgoing edges from any bubble will be both sufficient and provable.


\noindent 
The structure of the algorithm will be very similar to the primal-dual algorithm for \minvcover
from the previous section. The algorithm starts with an empty set of selected edges, and all-zero
bubbles. While the algorithm for \minvcover selected in one iteration a single variable, increased it
as much as possible, and selected one from the filled vertices, our algorithm will in one iteration
increase several dual variables in parallel, and admits one of the filled edges to the solution.


\noindent
To what bubbles we are going to add charge in a single iteration? Clearly, they must correspond to hungry sets
(remember, no charge ever to sets that are not hungry). Moreover, since all edges selected so far are 
already full, a bubble
around a hungry set from which already leaves at least one selected edge, cannot be charged more. 
This leaves us only the possibility to put charge to bubbles around hungry sets (there must be some
edge leaving them in any feasible solution), with  no selected edge that leaves them; such bubbles
will be called {\em unhappy}. These are the sets that violate the feasibility of program  (\ref{eq:minsforest:P})).
Potentially, there may be exponentially many unhappy sets, which could pose problems, because the algorithm
may explicitly store only polynomially many dual variables. However, it is easy to see that


\begin{lema}
  Unhappy sets that are minimal with respect to inclusion are the connected components of the graph 
  induced by the selected edges.
\end{lema}

\begin{dokaz}
  A set if unhappy if it is hungry and there is no selected edge leaving it. Hence any unhappy set
  is a union of several connected components of the graph induced by selected edges. An inclusion-minimal
  unhappy set is then a single connected component.
\end{dokaz}

\noindent
The algorithm increases in every iteration the dual variables corresponding to the unhappy connected
components. At the beginning there are no selected edges, and so the unhappy components are singleton 
bubbles around vertices that need to be connected. For the instance from the introductory example, these
are the sets  $\{u\},\{v\},\{s\},\{t\},\{p\},\{q\},\{x\},\{z\}$.
The algorithm increases the corresponding dual variables until some edges become full. In our case,
if the dual variables are increased to 1, edges $\{a,u\}$ and $\{t,p\}$ become full.

\begin{myfig}{\textwidth}{ovl/steiner-01}
\end{myfig}


\noindent
In the next iteration the unhappy components are $\{a,u\},\{s\},\{v\},\{t,p\},\{q\},\{z\},\{x\}$,
and the algorithm increases their charge by 2, thus filling the edges $\{s,b\}$ and $\{g,z\}$.
New unhappy components are formed, and the computation continues as follows:


\begin{minipage}[t]{0.5\textwidth}
\vskip 0pt
\begin{myfig}{\textwidth}{ovl/steiner-02}
\end{myfig}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\vskip 0pt
\begin{myfig}{\textwidth}{ovl/steiner-03}
\end{myfig}
\end{minipage}



%%%%%%%%%%%%%

\begin{minipage}[t]{0.5\textwidth}
\vskip 0pt
\begin{myfig}{\textwidth}{ovl/steiner-04}
\end{myfig}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\vskip 0pt
\begin{myfig}{\textwidth}{ovl/steiner-05}
\end{myfig}
\end{minipage}

\noindent
The algorithm terminates if all the connected components formed by the selected edges are happy. 
For an ease of presentation, it is easier to suppose that in each iteration exactly one 
edge is selected. If several edges connecting different components are filled, the subsequent iterations
increase the charge by zero amount; of course, an actual implementation would avoid this.

\noindent
The algorithm, as presented so far, has one tiny problematic detail: it does not work at all. 
Consider, e.g. the following simple graph there the only connectivity requirement is  $r(u,v)=1$.

\begin{minipage}[t]{0.4\textwidth}
  \vskip 0pt
\begin{myfig}{0.9\textwidth}{svg/steiner-badstar}
\end{myfig}
\end{minipage}
\hfill
\begin{minipage}[t]{0.5\textwidth}
  \vskip 0pt
  \noindent
  At the beginning the unhappy components are  $\{u\}$ and  $\{v\}$ and the algorithm 
  assigns a charge 1 to both of them. This, however, fills the edges $\{z_i,u\}$, so the algorithm 
  subsequently selects all the edges, creating a solution with size  $\ell+3$. The optimal solution,
  however, has cost three. Let us try to save our algorithm with a quick fix: after the algorithm terminates,
  the solution is modified by removing all unnecessary edges; an edge is $e${\em necessary}
  if $F-\{e\}$ is not a feasible solution.
\end{minipage}


\noindent
The final algorithm looks as follows:

  \vskip 2ex
\hrule
  \begin{itemize}
    \item[1] $F:=\emptyset$, $y_{\{v\}}:=0$ for each $v\in V$
    \item[2] while there exists an unhappy connected component induced by edges from  $F$
      \begin{itemize}
        \item[3] increase $y_S$ for all sets $S$ corresponding to unhappy connected components
          from $F$ until some edge $e$ is filled
        \item[4] $F:=F\cup e$
      \end{itemize}
    \item[5] $F':=F$
    \item[6] for each edge $e\in F$
      \begin{itemize}
        \item[7] if $F-\{e\}$ is feasible, $F':=F'-\{e\}$
      \end{itemize}

  \end{itemize}
\hrule
\vskip 3ex
\noindent
First let us make sure that the algorithm is correct:
\begin{lema}
  \label{lm:steiner:corr}
  After the algorithm terminates, edges $F'$ form a feasible solution of the program (\ref{eq:minsforest:ILP}),
  and the values $y_S$ form a feasible solution of the program (\ref{eq:minsforest:D}).
\end{lema}

\begin{dokaz}
After the cycle on line 2 terminates, there is no unhappy connected component induced by the edges $F$. Since 
every unhappy set is a union of some connected components, $F$ contains an outgoing edge from
every hungry set, and so $F$ is a feasible solution of  (\ref{eq:minsforest:ILP}).
It remains to check that the lines 6 and 7 do not change this property.
Recall that an edge $e$ is necessary if $F-\{e\}$ is not feasible. We show that if all unnecessary
edges are removed at the same time from $F$, the remaining set $F'$ is feasible. This follow immediately
from the observation that the edges from $F$ induce an acyclic graph: indeed, on line 3 only $y_S$ values for
a connected components $S$ are increased, so an edge inside a connected component can not become full. Hence,
every selected edge connects two components, and so cannot induce a cycle. For any two vertices  $u$, $v$
that must be connected, i.e. $r(u,v)=1$, there is exactly one $u-v$ path in $F$, so all edges on it are 
necessary, and thus end up in $F'$.

On the other hand, the values $y_S$ are only increased to the extent that no edge is overcharged, so
during the whole computation, the $y_S$ are a feasible solution of  (\ref{eq:minsforest:D}).
\end{dokaz}

\noindent 
We see that after termination of the algorithm, we have some feasible solution $F'$, and some
feasible solution \bm{y} of the dual program. In order to derive a guarantee on the approximation ratio, we need
to compare the value of the solution $F'$ with the value of the optimum. We, of course, don't know the exact optimum,
but we know it is at least as big as any feasible solution of the dual program. So in order to prove
2-approximation it is sufficient to show that the value of $F'$ is at most twice the value of \bm{y}, i.e.


\begin{veta}
  $$\sum_{e\in F'}\omega_e \le 2 \sum_{S\subseteq V}y_Sf(S)$$
\end{veta}

\begin{dokaz}
  Since we never put any charge to non-hungry sets, $f(S)=0$ implies $y_S=0$, so we want to prove
  $$\sum_{e\in F'}\omega_e \le 2 \sum_{S\subseteq V}y_S.$$ 
  Now let us introduce the following notation: for some sets $W\subseteq E$ and  $S\subseteq V$,
  denote  $\deg_W(S):=|W\cap\partial(S)|$, i.e. the number of edges from $W$ that have one endpoint in
  $S$ and the other outside $S$. Every edge that was selected to $F$ (and hence to $F'$) is full, since
  the charge is never decreased, and the edge was full when if was included in $F$; for the left-hand side we thus
  get
  $$\sum_{e\in F'}\omega_e=\sum_{e\in F'}\left(\sum_{S:e\in\partial(S)}y_S\right)
  =\sum_{S\subseteq V}\Deg_{F'}(S)y_S
  .$$
  We need to prove
  \begin{equation}
    \label{eq:sforest:1}
    \sum_{S\subseteq V}\Deg_{F'}(S)y_S\le 2 \sum_{S\subseteq V}y_S
  \end{equation}
  We show this by induction on the number of the iterations of the algorithm. At the beginning, the statemet
  (\ref{eq:sforest:1}) holds trivially, since all the values $y_S=0$. Consider now some iteration $\ell$, in which 
  all the values $y_S$ of the unhappy sets $S$ were increased by $\Delta$. How this changed the (\ref{eq:sforest:1})?
  Each unhappy component $S$ adds  $\Delta\Deg_{F'}(S)$ to the left-hand side, and  $2\Delta$ to the right-hand side.
  In order to prove  (\ref{eq:sforest:1}), we show that the increase to the right-hand side is bigger than the
  increase to the left-hand side, i.e.

  \begin{equation}
     \label{eq:sforest:2}
  \Delta\left(\sum_{S\in\S_\ell}\Deg_{F'}(S)\right)
  \le 2\Delta|\S_\ell|,
\end{equation}
where $\S_\ell$ is the family of all unhappy components in this iteration. The inequality  (\ref{eq:sforest:2})
can be rewritten as 
  $$\frac{\sum_{S\in\S_\ell}\Deg_{F'}(S)}{|\S_\ell|}\le2,$$
i.e. we need to show that the average degree of an unhappy component, with respect to $F'$, is at most 2.
If we denote by $F_\ell$ the set of selected edges at the beginning of iteration $\ell$, the situation looks as 
follows:


\begin{myfig}{0.8\textwidth}{svg/steiner1}
  The black edges are the final solution $F'$. The blue edges are the currently selected edges $F_\ell$.
  The highlighted components are unhappy.
\end{myfig}

\noindent
Both the edges $F'$ and $F_\ell$ are subsets of $F$. Since $F$ induce a forest (see the proof of 
Lemma~\ref{lm:steiner:corr}), $F'\cup F_\ell$ induce a forest, too. If every connected component of $F_\ell$
is contracted to a single vertex (the yellow blobs on the previous figure), a forest $H$ is obtained, whose
vertices correspond to connected components of $F_\ell$, and the edges are formed by the edges of $F'$. To prove
the inequality  (\ref{eq:sforest:2}) means to prove that the average degree of a vertex in $H$ is at most two,
where the average is taken only from the vertices that correspond to unhappy components.
If not for this last restriction, we could end the proof here: any forest with $n$ vertices has at most
$n-1$ edges, implying that the sum of the degrees is at most $2(n-1)$, and the average is thus less than 2.
However, we need to bound the average degree of the vertices corresponding to unhappy components only.
In order to do so, we show that the vertices corresponding to {\em happy} sets cannot have degree 1: they
are either isolated, or have degree at least two. After we prove this, the proof will be concluded as follows:
no unhappy component has degree 0 in $H$, because there is at least one outgoing edge in $F'$ from it.
Isolated vertices in $H$ thus correspond to happy components. If we remove isolated vertices from $H$, 
we obtain a new forest $H'$: it is sufficient to prove that the average degree of unhappy components in $H'$
is at most 2. However, the average degree of all components (vertices of $H'$) is at most 2, and
every happy component has degree at least two. So it must be that the average degree of unhappy components
cannot be more than 2.

To conclude we show that a happy component cannot have degree 1 in $H$. Suppose by contradiction that 
there exists a connected component $C$ in $F_\ell$, such that there is exactly on outgoing edge $e$ from
$C$ in $F'$. The edge $e$ survived from $F$ to $F'$, because it is necessary, i.e. it is part of the
unique path in $F$ connecting two vertices $u$, $v$ that must be connected ($r(u,v)=1$).
However, if $e$ is the only outgoing edge from $C$ in $F'$, then the vertices $u$, $v$ must be located one in $C$
and the other outside $C$, But then $C$ cannot be happy in $F_\ell$, a contradiction.

\end{dokaz}



